{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".output_png {\n",
       "    display: table-cell;\n",
       "    text-align: center;\n",
       "    vertical-align: middle;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports and declarations\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"image.cmap\"] = \"binary_r\"\n",
    "import sys\n",
    "sys.path.append(\"../../semi-supervised\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoder\n",
    "\n",
    "This notebook shows how to use a Variational AutoEncoder to model the data distribution over MNIST-digits $p(x)$. We will then learn a latent distribution $q_{\\theta}(z \\mid x)$ based on the digits. The M1 model in (Kingma 2014) describes the semi-supervised classifier as mapping latent representations to their label using a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_bernoulli = lambda img: transforms.ToTensor()(img).view(-1).bernoulli()\n",
    "\n",
    "mnist = datasets.MNIST('./', train=True, download=True, transform=flatten_bernoulli)\n",
    "mnist_val = datasets.MNIST('./', train=False, download=True, transform=flatten_bernoulli)\n",
    "\n",
    "unlabelled = torch.utils.data.DataLoader(mnist, batch_size=100, shuffle=True, num_workers=2)\n",
    "validation = torch.utils.data.DataLoader(mnist_val, batch_size=100, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAB7CAYAAABQIQWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12ca88710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1, 10, figsize=(10, 2))\n",
    "images, labels = next(iter(unlabelled))\n",
    "\n",
    "for i in range(10):\n",
    "    axarr[i].imshow(images[i].numpy().reshape(28, 28))\n",
    "    axarr[i].set_title(labels[i])\n",
    "    axarr[i].axis(\"off\")\n",
    "    \n",
    "f.suptitle(\"Data samples after Bernoulli transform\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational autoencoder is defined in terms of its input dimensionality along with the dimensionality of the latent space. In this example we have exactly one hidden layer both the encoder and decoder with 128 neurons. In order to train the model, we use variational inference by fitting a variational distribution $q(z | X)$ to the encoder. The full objective is given by: \n",
    "\n",
    "$${\\mathcal {L}}(\\phi,\\theta, X) = D_{KL}(q_{\\phi}( z | X ) \\ || \\ p(z))-\\mathbb {E}_{q_{\\phi }(z |X )}\\log p_{\\theta }(X | z )$$\n",
    "\n",
    "Where $\\phi$ and $\\theta$ are parameters of the encoder and decoder network respectively. The objective means that we want to make maximise the models ability to recreate the input given a learned latent representation $x$ while keeping $q(z | X)$ similar to some known distribution.\n",
    "\n",
    "For this problem we choose $q(z | X)$ to be similar to a standard normal distribution by using `kl_divergence_normal` and we measure the pixelwise binary cross entropy as our likelihood function. This leads to the model and loss function as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder (\n",
       "  (encoder): Encoder (\n",
       "    (hidden): ModuleList (\n",
       "      (0): Linear (784 -> 128)\n",
       "    )\n",
       "    (mu): Linear (128 -> 32)\n",
       "    (log_var): Linear (128 -> 32)\n",
       "  )\n",
       "  (decoder): Decoder (\n",
       "    (hidden): ModuleList (\n",
       "      (0): Linear (32 -> 128)\n",
       "    )\n",
       "    (reconstruction): Linear (128 -> 784)\n",
       "    (output_activation): Sigmoid ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.vae import VariationalAutoencoder\n",
    "from inference.loss import VariationalInference, kl_divergence_normal\n",
    "\n",
    "model = VariationalAutoencoder([28*28, 32, [128]])\n",
    "\n",
    "if cuda: model = model.cuda()\n",
    "\n",
    "def binary_cross_entropy(r, x):\n",
    "    return F.binary_cross_entropy(r, x, size_average=False)\n",
    "    \n",
    "objective = VariationalInference(binary_cross_entropy, kl_divergence_normal)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 27040.676\n",
      "Epoch: 5 loss: 18902.938\n",
      "Epoch: 10 loss: 16547.492\n",
      "Epoch: 15 loss: 15040.252\n",
      "Epoch: 20 loss: 14514.845\n",
      "Epoch: 25 loss: 14359.819\n",
      "Epoch: 30 loss: 14866.107\n",
      "Epoch: 35 loss: 13494.111\n",
      "Epoch: 40 loss: 12870.421\n",
      "Epoch: 45 loss: 12731.500\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "for epoch in range(50):\n",
    "    for u, _ in unlabelled:\n",
    "        u = Variable(u)\n",
    "\n",
    "        if cuda: u = u.cuda()\n",
    "\n",
    "        reconstruction, (_, z_mu, z_log_var) = model(u)\n",
    "        # Equation 3\n",
    "        L = objective(reconstruction, u, z_mu, z_log_var)\n",
    "\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        l = L.data[0]\n",
    "        print(\"Epoch: {0:} loss: {1:.3f}\".format(epoch, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Now that we have constructed a distribution $p(z | x)$ and are able to transform our data $x$ into latent representation $z$, we can perform classification. In Kingma 2014, it is suggested to use an SVM. We will instead use a simple multilayer perceptron, which should work as the latent space is of lower dimensionality than the input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(32, 10),\n",
    "    nn.Softmax())\n",
    "\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss: 1.629, accuracy: 0.881\n",
      "Epoch: 10 loss: 1.578, accuracy: 0.901\n",
      "Epoch: 20 loss: 1.594, accuracy: 0.899\n",
      "Epoch: 30 loss: 1.639, accuracy: 0.903\n",
      "Epoch: 40 loss: 1.563, accuracy: 0.905\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    for x, y in unlabelled:\n",
    "\n",
    "        x = Variable(x)\n",
    "        y = Variable(y)\n",
    "\n",
    "        if cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        _, (z, _, _) = model(x)\n",
    "        logits = classifier(z)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # Check validation accuracy\n",
    "        accuracy = []\n",
    "        for x, y in validation:\n",
    "            _, (z, _, _) = model(Variable(x))\n",
    "            logits = classifier(z)\n",
    "            _, prediction = torch.max(classifier(z), 1)\n",
    "\n",
    "            accuracy += [torch.mean((prediction.data == y).float())]\n",
    "\n",
    "        print(\"Epoch: {0:} loss: {1:.3f}, accuracy: {2:.3f}\".format(epoch, loss.data[0], np.mean(accuracy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get around 90% accuracy from just running it for a few epochs. It is expected as we get more unsupervised data, the model will perform better. However, being an extremely simple model, we can never achieve great performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
