{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "cuda = torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../../semi-supervised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "The variational autoencoder (VAE) was described in its current form in [Kingma 2013](https://arxiv.org/abs/1312.6114). The model consists of an encoder/inference network $q_{\\phi}(z|x)$ and a decoder/generative network $p_{\\theta}(x|z)$. The main idea is that it is possible to both reconstruct and generate samples from from some input distribution by learning a variational distribution over the latent variable $z$.\n",
    "\n",
    "The VAE therefore has a bottleneck structure, where the input $x$ is encoded into a latent variable $z$. New data can then be generated by feeding a latent code into the generator network - $\\widehat{x} \\sim p_{\\theta}(z|x)$.\n",
    "\n",
    "Below we will instantiate a new variational autoencoder with this bottleneck structure consisting of a 2-layer encoder network turning an input MNIST image into a latent code: $784 \\to 256 \\to 128 \\to 32$. We also have a decoder that performs the operation in reverse: $32 \\to 128 \\to 256 \\to 784$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalAutoencoder(\n",
       "  (encoder): Encoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=784, out_features=256)\n",
       "      (1): Linear(in_features=256, out_features=128)\n",
       "    )\n",
       "    (sample): GaussianSample(\n",
       "      (mu): Linear(in_features=128, out_features=32)\n",
       "      (log_var): Linear(in_features=128, out_features=32)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (hidden): ModuleList(\n",
       "      (0): Linear(in_features=32, out_features=128)\n",
       "      (1): Linear(in_features=128, out_features=256)\n",
       "    )\n",
       "    (reconstruction): Linear(in_features=256, out_features=784)\n",
       "    (output_activation): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models import VariationalAutoencoder\n",
    "from layers import GaussianSample\n",
    "model = VariationalAutoencoder([784, 32, [256, 128]])\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the middle most layer consists of a `GaussianSample` layer, in which we turn the input digit into the parameters of a Normal distribution with parameters $\\mu$ and $\\sigma$. This allows us to use the *reparametrization trick* to sample from this distribution to introduce stochasticity into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample 0.36 drawn from N(1.24, 4.57)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "gaussian = GaussianSample(10, 1)\n",
    "z, mu, log_var = gaussian(Variable(torch.ones(1, 10)))\n",
    "\n",
    "print(f\"sample {float(z.data):.2f} drawn from N({float(mu.data):.2f}, {float(log_var.exp().data):.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "How do we go about training a variational autoencoder then? We want to model the data distribution $p(x)$, from here we can introduce a variational distribution $q(z|x)$ by multiplying and dividing by this distribution. Now we employ Jensen's inequality to move the logarithm inside the integral, we can do this because $\\log$ is concave and because $q(z|x)$ is a probability distribution. From here on we just rearrange and we see that a lower bound on the marginal probability of the data $p(x)$ is just an expectation over the likelihood of the data minus the KL-divergence between the variational distribution and a prior $p(z)$.\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(x) &= \\log \\int p(x, z) \\ dz = \\log \\int q(z|x) \\frac{p(x, z)}{q(z|x)} \\ dz\\\\\n",
    "          &\\geq \\int q(z|x) \\log \\frac{p(x, z)}{q(z|x)} \\ dz = \\int q(z|x) \\log p(x|z) + \\log \\frac{p(z)}{q(z|x)} \\ dz\\\\\n",
    "          &= \\int q(z|x) \\log p(x|z) \\ dz + \\int q(z|x) \\log \\frac{p(z)}{q(z|x)} \\ dz\\\\\n",
    "          &= \\mathbb{E}_{q(z|x)} [\\log p(x|z)] - KL(q(z|x)||p(z)) = \\mathcal{L}(x)\n",
    "\\end{align}\n",
    "\n",
    "To make things even more concrete, we show how we can go from this equation to an actual algorithm. Recall that the expectation is just an arithmetic mean, which can be approximated using Monte-Carlo samples. In fact for most applications we can do with just a single sample, even though this provides infinite variance.\n",
    "\n",
    "$$\\mathbb{E}_{q(z|x)} [\\log p(x|z)] = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{i=1}^{N} \\log p(x_i|z_i) \\approx \\frac{1}{M} \\sum_{i=1}^{M} \\log p(x_i|z_i)$$\n",
    "\n",
    "As you can see the likelihood is just the log probability of the data given the latent variable, but the latent variable is itself derived from the data - we can just use the reconstruction error! In the MNIST case, it is most fitting to use the Bernoulli / binary cross entropy.\n",
    "\n",
    "Finally, the second term is the Kullback-Leibler divergence. It states that whatever distribution we learn over $q(z|x)$ can never be very far from the prior $p(z)$. This is both good and bad news. Ideally we want a reconstruction that is as good as possible, i.e. only relying on the likelihood, which will only occur if $q(z|x) = p(z)$. This will never happen in practice as the q-distribution will be very complex in order to produce latent variables that result convincing samples. On the plus side, the KL-term acts as a regularizer that pulls the distribution towards the prior, which is the whole reason why we can create samples.\n",
    "\n",
    "This term can either be computed analytically, or by sampling similarly to the way we did it in the expectation, which you can see in the printed doc string below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Computes the KL-divergence of\n",
      "        some element z.\n",
      "\n",
      "        KL(q||p) = -âˆ« q(z) log [ p(z) / q(z) ]\n",
      "                 = -E[log p(z) - log q(z)]\n",
      "\n",
      "        :param z: sample from q-distribuion\n",
      "        :param q_param: (mu, log_var) of the q-distribution\n",
      "        :param p_param: (mu, log_var) of the p-distribution\n",
      "        :return: KL(q||p)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(model._kld.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import get_mnist\n",
    "\n",
    "_, train, validation = get_mnist(location=\"./\", batch_size=64)\n",
    "\n",
    "def binary_cross_entropy(r, x):\n",
    "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tL: 125.23\n",
      "Epoch: 10\tL: 125.01\n",
      "Epoch: 20\tL: 124.72\n",
      "Epoch: 30\tL: 124.59\n",
      "Epoch: 40\tL: 124.36\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (u, _) in train:\n",
    "        u = Variable(u)\n",
    "\n",
    "        if cuda: u = u.cuda(device=0)\n",
    "\n",
    "        reconstruction = model(u)\n",
    "        \n",
    "        likelihood = -binary_cross_entropy(reconstruction, u)\n",
    "        elbo = likelihood - model.kl_divergence\n",
    "        \n",
    "        L = -torch.mean(elbo)\n",
    "\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += L.data[0]\n",
    "\n",
    "    m = len(train)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch: {epoch}\\tL: {total_loss/m:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the generative model\n",
    "\n",
    "Now that we have trained the network, we can begin to sample from it. We simply give it some random noise distributed according to the prior $p(z) = \\mathcal{N}(0, I)$ and send it through the decoder. This process generates a slew of samples that look like they come from the original distribution $p(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "x_mu = model.sample(Variable(torch.randn(16, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABA0AAABVCAYAAAAi7x1kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np4ExxhhjjDHGGGOa8v/BdcWuZRwjoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x124e49320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1, 16, figsize=(18, 12))\n",
    "\n",
    "samples = x_mu.data.view(-1, 28, 28).numpy()\n",
    "\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.imshow(samples[i])\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
