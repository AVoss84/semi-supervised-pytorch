{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports and declarations\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.rcParams[\"image.cmap\"] = \"binary_r\"\n",
    "sys.path.append(\"../../semi-supervised\")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Generative Model\n",
    "\n",
    "In this notebook we will run the Deep Generative Model as described in (Kingma 2014). The model builds on a standard variational autoencoder by adding label information during the inference. The main gist of the model is that we utilise label information when available, and marginalise over all labels when unavailable.\n",
    "\n",
    "Here we use a limited subset of MNIST to make training faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.limitedmnist import LimitedMNIST\n",
    "from utils import generate_label, onehot\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "n = len(labels)\n",
    "\n",
    "# Load in data\n",
    "mnist_lab = LimitedMNIST('./', train=True, transform=torch.bernoulli, target_transform=onehot(n), digits=labels, fraction=0.005)\n",
    "mnist_ulab = LimitedMNIST('./', train=True, transform=torch.bernoulli, target_transform=onehot(n), digits=labels, fraction=1.0)\n",
    "mnist_val = LimitedMNIST('./', train=False, transform=torch.bernoulli, target_transform=onehot(n), digits=labels)\n",
    "\n",
    "# Unlabelled data\n",
    "unlabelled = torch.utils.data.DataLoader(mnist_ulab, batch_size=100, shuffle=True, num_workers=2)\n",
    "\n",
    "# Validation data\n",
    "validation = torch.utils.data.DataLoader(mnist_val, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Labelled data\n",
    "labelled = torch.utils.data.DataLoader(mnist_lab, batch_size=100, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAB7CAYAAABQIQWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFTtJREFUeJzt3XtsU/X7B/B353QM2q3d2GDM0slQBOXiLeiQ26KIGgZ4\nAxRERREjiihGYCD7CsbgBQwoTmFuXoKiJmjkoigBiagDRURBI0zYZGMjArvUMem25/cH2fl1o+3a\nnXbt+ez9Skjozuk5z3M+5/Kcz/m0NYmIgIiIiIjaJCrcARAREREZGYspIiIiIh1YTBERERHpwGKK\niIiISAcWU0REREQ6sJgiIiIi0oHFFJGCRowYgTVr1rTrOt944w1069YNZrMZJ06caNd1R6K0tDR8\n/fXXAICcnBxMnjwZAFBSUgKz2YyGhoZwhufR6dOnMWbMGMTHx+POO+8MdzhEhsFiipSSlpaG2NhY\nWCwWWK1WZGRkIDc3F42NjX69/8iRIzCZTKivrw9xpGpxuVx48sknsWXLFjidTiQmJsJkMuHQoUNB\nXY/JZEKXLl1gNpvRtWtXTJo0CZWVlUFdR6j17NkTTqcT5513nsfp7kVYe/vkk09QUVGBEydO4OOP\nPw5LDERGxGKKlPP555+jpqYGxcXFmDt3LpYuXYpp06aFOyylVVRUoK6uDpdddllQluermP3ll1/g\ndDrx119/4dSpU8jJyQn6OiJZKOMuLi7GJZdcgujo6IDfa9TtSRQMLKZIWfHx8cjKysK6devwzjvv\n4LfffgMAbNy4EVdccQXi4uJgt9ubXYyHDRsGALBarTCbzfj+++9RVFSEzMxMJCYmomvXrrjnnnu8\n9oaICGbPno3k5GTExcWhf//+fq23qUcsPz8fdrsdNpsNubm52L17NwYMGACr1YqZM2dq8xcUFGDI\nkCGYOXMm4uPjcemll2Lr1q1et8Xbb7+Nvn37wmaz4aabbkJxcXGr8baUn5+Pvn37wmKxoFevXnjz\nzTcBAH/++Sf69OmjbbfMzExtOw4cOBBmsxnr1q0DAGzYsAGDBg3Seg337dunLT8tLQ1Lly7FgAED\n0KVLl1YvznFxccjKysKBAwe0v1VVVWHatGlISUlBamoqFixYoD1Oa9pms2fPRmJiInJyclBQUIDr\nr78ec+bMgc1mw0UXXYTNmzdryysrK0NWVhYSEhLQu3dvrF69Wpt23333YcGCBdrr7du348ILL/QZ\nM+C793PKlCkoKSnBmDFjYDab8eKLL2rz5+XloWfPnsjMzAQA3HnnnejevTvi4+MxbNgw7N+/v1ls\njz76KG699VZYLBYMHjwYRUVFALy3+aJFi/Dcc89h3bp1MJvNyMvLQ2NjI5YsWQKHw4Hk5GTce++9\nqKqqapaHe1yB7sdEyhAihTgcDvnqq6/O+bvdbpdVq1aJiMi2bdtk37590tDQIL/88oskJyfL+vXr\nRUTk8OHDAkBcLpf23oMHD8qWLVukrq5Ojh8/LkOHDpVZs2Z5XP8XX3whV155pZw6dUoaGxvlwIED\nUlZW5vd6H374YTl9+rR8+eWXEhMTI2PHjpWKigo5evSoJCUlyfbt20VEJD8/X8477zxZtmyZnDlz\nRj788EOJi4uTEydOiIjI8OHDZfXq1SIi8umnn0p6erocOHBAXC6XLF68WK677rpW421pw4YNcujQ\nIWlsbJTt27dLbGys/PTTT163GwA5ePCg9nrPnj2SlJQkP/zwg9TX10tBQYE4HA6pq6vT2m7gwIFS\nUlIitbW1HmNwX+bJkyflxhtvlIULF2rTx40bJ9OnTxen0ykVFRVyzTXXSG5ubrNttmLFCnG5XFJb\nWyv5+fkSHR0tb731ltTX18uqVaskJSVFGhsbRURk6NCh8sgjj8jp06fl559/lq5du8rWrVtFRGTq\n1KmSnZ2trXvbtm2SmpqqvXbfFxctWiT33HOP123lruU+3DT/lClTxOl0atsmLy9Pqqurpa6uTmbN\nmiUDBw7U3jN16lRJSEiQwsJCcblccvfdd8uECRNExHebu8fZtI709HQpKiqSmpoaGT9+vEyePNlr\nXIHux0SqYDFFSvFWTA0ePFiWLFni8T2zZs2SJ554QkRav9CJiKxfv14GDRrkcdrWrVvl4osvlu+/\n/14aGhp8xuppvUePHtWmJyQkyIcffqi9vu2222T58uUicrYwcL/oi4hcc8018u6774pI82Jq9OjR\nsmbNGm2+hoYGiY2NlSNHjgQUb0tjx46VV199tVn8voqpGTNmyIIFC5ot45JLLtEurA6HQ/Ly8nyu\nE4BYLBaJj4+XqKgo6dOnj7bNysvL5YILLmhWiK1du1ZGjBghIme3md1ub7a8/Px8SU9P117/+++/\nAkCOHTsmJSUlEhUVJdXV1dr0uXPnytSpU0Wk/YupoqIir9vl1KlTAkAqKyu12KZNm6ZN37hxo/Tp\n00dEfO+jLYupzMxMef3117XXf/zxh0RHR4vL5fIYV6D7MZEq+JiPOoTS0lIkJCQAAAoLCzFy5Egk\nJSUhPj4eubm5+Oeff7y+t6KiAhMnTkRqairi4uIwefJkr/NnZmZi5syZePTRR5GcnIzp06ejurra\n7/V269ZN+39sbOw5r51Op/Y6NTUVJpNJe+1wOFBWVnZOTMXFxZg1axasViusVisSEhIgIigtLfUZ\nb0ubN2/Gtddei4SEBFitVmzatMnndvMUxyuvvKLFYbVa8ffffzeL2W63t7qcPXv2oLKyEnV1dXjk\nkUcwdOhQ1NXVobi4GC6XCykpKdryH374YRw/ftzn8rt37679v3PnzgAAp9OJsrIyJCQkwGKxaNMd\nDgdKS0v9zjmY3GNvaGjA3LlzkZ6ejri4OKSlpQFAs/ZomVfTvhNIm5eVlcHhcGivHQ4H6uvrUVFR\n4TGuJoHsx0QqYDFFytu9ezdKS0tx/fXXAwDuvvtuZGVl4e+//0ZVVRVmzJgBEQGAZsVJk/nz58Nk\nMuHXX39FdXU13n//fW1+Tx5//HH89NNPOHDgAP7880+89NJLra63LUpLS5u9v6SkBD169DhnPrvd\njjfffBOVlZXav9OnTyMjI8NnvO7+++8/3H777ZgzZw4qKipQWVmJW265JaD47XY7srOzm8VRW1uL\nSZMmafN42v7enH/++XjwwQdx+PBh/Pbbb7Db7YiJicE///yjLb+6urrZWKJAlt+jRw+cPHkSNTU1\n2t9KSkqQmpoKAOjSpQtqa2u1aeXl5X4v2xdvMbr/fe3atfjss8/w9ddfo6qqCkeOHAEAv9vDnzYH\nzm6DpvF1wNn8o6OjmxVHgWxTIlWxmCJlVVdXY8OGDZg4cSImT56M/v37AwBqamqQkJCATp06Ydeu\nXVi7dq32nqSkJERFReGvv/7S/lZTUwOz2Yz4+HiUlpZ6vfAAZwu3wsJCuFwudOnSBZ06dUJUVFSr\n622L48ePY8WKFXC5XPj444/x+++/45ZbbjlnvhkzZuCFF17QioqqqirtY+++4nV35swZ/Pfff0hK\nSkJ0dDQ2b96MLVu2+IyvW7duzbbjQw89hNzcXBQWFkJE8O+//2Ljxo3NipVANDQ0ID8/H7GxsejV\nqxdSUlIwatQoPPXUU6iurkZjYyOKiorwzTfftGn5drsdGRkZmDdvHurq6rBv3z7k5eVp3xc1aNAg\nbNq0CSdPnkR5eTleffXVNq2npZbbzZOamhrExMQgMTERtbW1mD9/vt/L97fNAWDSpElYvnw5Dh8+\nDKfTifnz52PChAlt+rQfkcpYTJFyxowZA4vFArvdjueffx5PPvkk8vPztemrVq3Cs88+C4vFguee\new533XWXNq1z587Izs7GkCFDYLVa8cMPP2DRokXYs2cP4uPjceutt+K2227zuu7q6mo89NBDsNls\ncDgcSExMxNNPP93qetti8ODBOHjwILp27Yrs7Gx88sknSExMPGe+8ePH45lnnsHEiRMRFxeHyy+/\nXPvEmq943VksFqxYsQJ33XUXbDYb1q5di6ysLJ/x5eTkYOrUqbBarfjoo49w9dVXY/Xq1Zg5cyZs\nNht69+6NgoKCgPNu+oSgzWbDO++8g/Xr12uPcN99912cOXMG/fr1g81mwx133IFjx44FvI4mH3zw\nAY4cOYIePXpg/Pjx+N///ocbbrgBwNlP3g0cOBBpaWkYNWoUJkyY0Ob1uJs3bx6WLFkCq9WKl19+\n2eM89957LxwOB1JTU9GvXz9ce+21fi/f3zYHgAceeABTpkzBsGHDcNFFF6FTp05YuXJlm/IiUplJ\n9DxnIKKwKCgowJo1a/Dtt9+GOxQiog6PPVNEREREOrCYIiIiItKBj/mIiIiIdGDPFBEREZEOLKaI\niIiIdGAxRURERKQDiykiIiIiHVhMEREREenAYoqIiIhIBxZTRERERDqwmCIiIiLSgcUUERERkQ4s\npoiIiIh0YDFFREREpAOLKSIiIiIdWEwRERER6cBiioiIiEgHFlNEREREOhi+mNqxYwfGjh0Lh8MB\nk8mEJUuWhDukoFI9P4A5qmLTpk0YNGgQYmJikJaWhmXLloU7pKBjjsb23nvv4aqrroLNZkNsbCz6\n9u2LZcuWQUTCHVpQ8XzT/gxfTDmdTvTr1w8vvvgiunfvHu5wgk71/ADmqIIff/wRY8eOxc0334y9\ne/ciJycH8+fPR25ubrhDCxrmaHzJyclYuHAhvvvuO+zfvx9z587FwoULsWLFinCHFlQ834SBKMTh\ncMjixYvDHUbIqJ6fCHM0qkmTJsl1113X7G9z5swRh8MRnoBCgDmqady4cTJu3LhwhxEyPN+0D8P3\nTBFR+O3cuROjR49u9rfRo0ejuLgYR48eDVNUwcUc1cixiYhg165d2LlzJ0aOHBnucCgAkbifspgi\nIt2OHTt2zuOEptfHjh0LR0hBxxzVyLGqqgpmsxkxMTHIyMjAY489hscffzzcYVEAInE/jQ7LWomI\niMLAYrFg7969qK2txXfffYd58+ahR48emDZtWrhDIwNjMUVEuqWkpKC8vLzZ3yoqKrRpKmCOauQY\nFRWF3r17AwAGDBiAU6dOITs7m8WUgUTifsrHfESk25AhQ/Dll182+9sXX3wBh8OBCy+8MExRBRdz\nVCPHlhobG1FXVxfuMCgAkbifGr5nyul04tChQwCAM2fOoLy8HHv37oXZbNbuPoxM9fwA5qhCjrNn\nz0ZGRgays7MxZcoUFBYWYuXKlVi+fHm4Qwsa5mh8ixYtwtChQ9GrVy+4XC7s2LEDS5cuxf333x/u\n0IKK55swCNvnCINk27ZtAuCcf8OHDw93aEGhen4izFEVGzZskAEDBsgFF1wgPXv2lFdeeSXcIQUd\nczS2J554QtLT06VTp05itVrlyiuvlNdee03q6+vDHVpQ8XzT/kwiin31KxEREVE74pgpIiIiIh1Y\nTBERERHpwGKKiIiISAcWU0REREQ6sJgiIiIi0qFdv2fKZDK15+qCzp8PPqqeo+r5AczRCJij+vkB\nzNEImONZ7JkiIiIi0oHFFBEREZEOLKaIiIiIdGAxRUQhJSJ+jTkgIjIqFlNEREREOrTrp/mIvGnq\nuTD6pz68ce+ZUTVHokjirTfUZDIpf77xl6dtZJRt4i32cLU7e6aIiIiIdFC2Z6pldWqUars1Rr6T\naInjaDoWI/cG+LuvGjE3f/i6249EvtrLfZqR98mOomUbtfW6ISIhbWcli6mOcpE2+glA78FhVKqe\nwP294Bopf183L56mGSk3b1Q4Hj090vG3LY3cdv4y0k15INeJ1ubhYz4iIiKiCKVkz5SvQWgUOVS9\nsyfja+tPZDS9z6g9HYGeN0P96ESPlnG5v/bUTi2nRWpeehjtuhiqeEPRxuyZIiIiItJByZ4pMoaO\nNGZKxd5SX/n4uuNzb3ej3f23JV4j9HTo2TeNkF9LHencoypfPcPhoGQx1REOECOduMj7PmnEC5Ev\ngeYRifnrGZyr6kXan0HcRqRqe3nS1psfI/F009raB0aClTsf8xERERHpoGTPlDtVKm5SjxEfc6ks\nmL0ukd7jEUhc7tsg0vMKlCp5qEZPL5q36d6GWgSrd5w9U0REREQ6KN8zpSr2ahCFRjCPK/e74Ugc\nH+aJP71P7vMYJa+O/i32FNovaWXPFBEREZEOSvVMdbQvYCMKB6P9Tltr2nvcTLh7lf0dj+IrRiOM\nNQo0RqPuv9609YtnI1Wkx6pUMeVJpDeAv1T94WZvwn3BoY4r1PteJO7XwYgp0m5cVRssH6iOnr8n\nHIBOREREFKGU7ZmKlLujUFGp50bFbwdvSeUcVdkPmwQzn1B/UaDeWFRrOz0irWdNr5b5GPX3IoP9\ngRCAA9CJiIiIIo4SPVMdbTyRivh83xiC0T4dqY19/bxFJGhrb4yRjteWubUWswo9VO45GqlHKtT7\nUyiXr0Qx1RFF+kERDCqc1JoY4aITah3pMVOk56o3lkjPzxf3OHlcdhyhbms+5iMiIiLSgT1TBqXS\nAHQyjrYMpPc1v0q9j+48PQZTKVcjPebzRZU8muj5TTtVtdc2Yc8UERERkQ6G75lS5Y6C1BbqXyxv\nL8E83oySc7CEK99QnCM7wnmXA7cjQyjOkaFoT/ZMEREREelg2J4pI3+apC1Ue7bvjepjTYzOUw+b\ne/sYcf9suc+Fajwi9+PIZdR91x9G2t98Xef8PX78acdQHIuGLaY6SnFBZBT+HotGOrkHU7jyDua5\n0kjn20AvmKpdU1Q9zjw9fo2EH7XmYz4iIiIiHQzbM2WkwYHBpNrdExlPoPugUY5PX1/m6G8OkTj8\nQO8HH1pr53Dn54m3bwAntQTStqHeT9kzRURERKSD4XqmIvl3rtpDR7nLUnlAqCp8/eaZasdlW8dp\nRMJ28NVOnnqoVDvuVO7NVzEnIDRtFuovujZcMaXygUH/T8X29XQgq5RnJBQOweJPARLI+yOdER/l\n+eIp3kgarBws3nKK5JgD4anIb60dW05vr23Bx3xEREREOhiuZ6qjU+WOozXMkyKJ+91uuO58g8XX\n3b47o+bXGtXyAdTMqaWWObaWc3tvE/ZMEREREelg2J6pjlCJE1FkcT/vqHAO8pWDCvkRtRf2TBER\nERHpwGKKiIiISAcWU0REREQ6sJgiIiIi0sEkKn1rIBEREVE7Y88UERERkQ4spoiIiIh0YDFFRERE\npAOLKSIiIiIdWEwRERER6cBiioiIiEgHFlNEREREOrCYIiIiItKBxRQRERGRDiymiIiIiHRgMUVE\nRESkA4spIiIiIh1YTBERERHpwGKKiIiISAcWU0REREQ6sJgiIiIi0oHFFBEREZEOLKaIiIiIdGAx\nRURERKQDiykiIiIiHVhMEREREenAYoqIiIhIBxZTRERERDr8H1gKqb31GJCRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c66aa58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f, axarr = plt.subplots(1, 10, figsize=(10, 2))\n",
    "images, labels = next(iter(labelled))\n",
    "_, labels = torch.max(labels, 1)\n",
    "\n",
    "for i in range(10):\n",
    "    axarr[i].imshow(images[i].numpy().reshape(28, 28))\n",
    "    title = labels[i] if labels[i] < 5 else \"Unknown\"\n",
    "    axarr[i].set_title(title)\n",
    "    axarr[i].axis(\"off\")\n",
    "    \n",
    "f.suptitle(\"Data samples after Bernoulli transform\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a Deep Generative Model by specifing the ratio between labelled and unlabelled data. We use variational inference to fit data to the model, like for the VAE. In this case, we also have label information so we use `VariationalInferenceWithLabels`. This objective is based on the ELBO objective described in (Kingma 2014).\n",
    "\n",
    "\n",
    "$$\\log p_{\\theta}(x, y) \\geq \\mathbb{E}_{q_{\\phi}(z|x, y)} [ \\log p_{\\theta}(x|y, z) + \\log p_{\\theta}(y) + \\log \\frac{p(z)}{q_{\\phi}(z|x, y)} ] = - \\mathcal{L}(x, y)$$\n",
    "\n",
    "Where the first term in the equation describes the likelihood function. The second can be viewed as a *prior* over the labels $y$, while the third part is just the KL-divergence we have already seen in VAEs. In this example, we choose a `discrete_uniform_prior` over the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepGenerativeModel (\n",
       "  (encoder): Encoder (\n",
       "    (hidden): ModuleList (\n",
       "      (0): Linear (128 -> 128)\n",
       "    )\n",
       "    (mu): Linear (128 -> 16)\n",
       "    (log_var): Linear (128 -> 16)\n",
       "  )\n",
       "  (decoder): Decoder (\n",
       "    (hidden): ModuleList (\n",
       "      (0): Linear (16 -> 128)\n",
       "    )\n",
       "    (reconstruction): Linear (128 -> 784)\n",
       "    (output_activation): Sigmoid ()\n",
       "  )\n",
       "  (classifier): Classifier (\n",
       "    (dense): Linear (784 -> 128)\n",
       "    (logits): Linear (128 -> 5)\n",
       "  )\n",
       "  (transform_x_to_h): Linear (784 -> 128)\n",
       "  (transform_y_to_h): Linear (5 -> 128)\n",
       "  (transform_y_to_z): Linear (5 -> 16)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.dgm import DeepGenerativeModel\n",
    "from inference.loss import VariationalInferenceWithLabels, kl_divergence_normal, discrete_uniform_prior\n",
    "\n",
    "# Numerical stability\n",
    "epsilon = 1e-7\n",
    "\n",
    "# DGM with a single hidden layer in both the encoder and decoder\n",
    "model = DeepGenerativeModel(ratio=len(mnist_ulab)/len(mnist_lab), dims=[28 * 28, n, 16, [128]])\n",
    "\n",
    "if cuda: model.cuda()\n",
    "\n",
    "def binary_cross_entropy(r, x):\n",
    "    return torch.sum((x * torch.log(r + epsilon) + (1 - x) * torch.log((1 - r) + epsilon)), dim=-1)\n",
    "\n",
    "def cross_entropy(y, logits):\n",
    "    return -torch.sum(y * torch.log(logits + epsilon), dim=1)\n",
    "    \n",
    "objective = VariationalInferenceWithLabels(binary_cross_entropy, kl_divergence_normal, discrete_uniform_prior)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model is a bit more complicated than for the standard VAE. We need to define two different cases based on whether the incoming data has labels or not. We use the definition for the ELBO given in the previous equation:\n",
    "\n",
    "$$- \\mathcal{L}(x, y) = \\mathbb{E}_{q_{\\phi}(z|x, y)} [ \\log p_{\\theta}(x|y, z) + \\log p_{\\theta}(y) + \\log \\frac{p(z)}{q_{\\phi}(z|x, y)} ]$$\n",
    "\n",
    "If labels are given, we calculate the ELBO along with the cross entropy.\n",
    "\n",
    "$$\\mathcal{L}(x, y) + \\alpha \\cdot \\mathbb{E}_{\\tilde{p}_l(x, y)}[- \\log q_{\\phi}(y|x)]$$\n",
    "\n",
    "However, if no labels are given, we must instead sum over all of the labels. In order to keep the equations balanced, we are required to calculate the entropy.\n",
    "\n",
    "$$\\sum_y q_{\\phi}(y|x)(- \\mathcal{L}(x, y)) + \\mathcal{H}(q_{\\phi}(y|x)) = -\\mathcal{U}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGMTrainer():\n",
    "    \"\"\"\n",
    "    Class for training Deep Generative Models.\n",
    "    :param model: Object of class `DeepGenerativeModel`\n",
    "    :param objective: Loss function for labelled data, e.g. `VariationalInferenceWithLabels`\n",
    "    :param optimizer: A PyTorch-enabled optimizer\n",
    "    :param cuda: Optional parameter whether to use CUDA acceleration\n",
    "    \"\"\"\n",
    "    def __init__(self, model, objective, optimizer, cuda=False):\n",
    "        self.model = model\n",
    "        self.objective = objective\n",
    "        self.optimizer = optimizer\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def calculate_loss(self, x, y=None):\n",
    "        \"\"\"\n",
    "        Given a semi-supervised problem (x, y) pair where y\n",
    "        is only occasionally observed, calculates the\n",
    "        associated loss.\n",
    "        :param x: Features\n",
    "        :param y: Labels (optional)\n",
    "        :returns L_alpha if labelled, U if unlabelled.\n",
    "        \"\"\"\n",
    "        is_unlabelled = True if y is None else False\n",
    "\n",
    "        x = Variable(x)\n",
    "        logits = self.model.classifier(x)\n",
    "\n",
    "        # If the data is unlabelled, sum over all classes\n",
    "        if is_unlabelled:\n",
    "            [batch_size, *_] = x.size()\n",
    "            x = x.repeat(n, 1)\n",
    "            y = torch.cat([generate_label(batch_size, i, n) for i in range(n)])\n",
    "\n",
    "        y = Variable(y.type(torch.FloatTensor))\n",
    "        \n",
    "        if self.cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        # Compute lower bound (the same as -L)\n",
    "        reconstruction, _, (z, z_mu, z_log_var) = self.model(x, y)\n",
    "        ELBO = self.objective(reconstruction, x, y, z_mu, z_log_var)\n",
    "\n",
    "        # In the unlabelled case calculate the entropy H and return U\n",
    "        if is_unlabelled:\n",
    "            ELBO = ELBO.view(logits.size())\n",
    "            loss = torch.sum(torch.mul(logits, ELBO - torch.log(logits)), -1)\n",
    "            loss = -torch.mean(loss)\n",
    "        # In the case of labels add cross entropy and return L_alpha\n",
    "        else:\n",
    "            loss = ELBO + self.model.beta * -cross_entropy(y, logits)\n",
    "            loss = -torch.mean(loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train(self, labelled, unlabelled):\n",
    "        \"\"\"\n",
    "        Trains a DGM model based on some data.\n",
    "        :param labelled: Labelled data loader\n",
    "        :param inlabelled: Unlabelled data loader\n",
    "        :return L, U: Final loss values.\n",
    "        \"\"\"\n",
    "        for (x, y), (u, _) in zip(labelled, unlabelled):\n",
    "            L = self.calculate_loss(x, y)\n",
    "            U = self.calculate_loss(u, None)\n",
    "\n",
    "            J = L + U\n",
    "\n",
    "            J.backward()\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        return L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Here you can optionally enable the use of `visdom` to visualise the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_visdom = True\n",
    "\n",
    "if use_visdom:\n",
    "    import visdom\n",
    "    vis = visdom.Visdom()\n",
    "\n",
    "class Visualiser():\n",
    "    def __init__(self):\n",
    "        self.loss = vis.line(X=np.array([0]), Y=np.array([0]), opts=dict(title=\"Training Loss\", xlabel=\"Epoch\"))\n",
    "        self.acc  = vis.line(X=np.array([0]), Y=np.array([0]), opts=dict(title=\"Accuracy\", xlabel=\"Epoch\"))\n",
    "\n",
    "    def update_loss(self, L, U):\n",
    "        vis.updateTrace(X=np.array([epoch]), Y=L.data.numpy(), win=self.loss, name=\"Labelled\")\n",
    "        vis.updateTrace(X=np.array([epoch]), Y=U.data.numpy(), win=self.loss, name=\"Unlabelled\")\n",
    "        \n",
    "    def update_accuracy(self, model):\n",
    "        accuracy = []\n",
    "        for x, y in validation:\n",
    "            _, prediction = torch.max(model.classifier(Variable(x)), 1)\n",
    "            _, y = torch.max(y, 1)\n",
    "\n",
    "            accuracy += [torch.mean((prediction.data == y).float())]\n",
    "\n",
    "        vis.updateTrace(X=np.array([epoch]), Y=np.array([np.mean(accuracy)]), win=self.acc)\n",
    "        \n",
    "    def update_images(self, model):\n",
    "        x, y = next(iter(validation))\n",
    "        input = Variable(x[:5])\n",
    "        label = Variable(y[:5].type(torch.FloatTensor))\n",
    "        x_hat, *_ = model(input, label)\n",
    "        images = x_hat.data.numpy().reshape(-1, 1, 28, 28)\n",
    "\n",
    "        vis.images(images, opts=dict(width=5*64, height=64, caption=\"Sample epoch {}\".format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controlling which dataloaders to use, we gather the losses into a single combined loss that we can backpropagate.\n",
    "\n",
    "$$\\mathcal{J}^{\\alpha} = \\mathcal{L}^{\\alpha} + \\mathcal{U}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "trainer = DGMTrainer(model, objective, optimizer)\n",
    "visual = Visualiser()\n",
    "\n",
    "for epoch in range(1000):\n",
    "    L, U = trainer.train(labelled, unlabelled)\n",
    "        \n",
    "    if use_visdom:\n",
    "        # Plot the last L and U of the epoch\n",
    "        visual.update_loss(L, U)\n",
    "        visual.update_accuracy(model)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            visual.update_images(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing training, your plots should look something like this:\n",
    "    \n",
    "![](../images/visdom-dgm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
